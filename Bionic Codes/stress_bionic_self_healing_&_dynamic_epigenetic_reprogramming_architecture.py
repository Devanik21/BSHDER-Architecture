# -*- coding: utf-8 -*-
"""Stress_Bionic Self-Healing & dynamic Epigenetic Reprogramming Architecture

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/164Y-C74CJHSPdtpgU3rP2-WJSvwKuCA2
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# --- MECHANISM 1: THE PHYSICS OF HEALING ---
class BionicPhysics:
    """Core physics engine for bionic reprogramming."""
    PHI = 1.6180339887  # Golden Ratio

    @staticmethod
    def stem_cell_genesis(shape, device):
        """Creates dormant 'stem' weights with high potential energy."""
        # Initialize with PHI scaling for rapid adaptation
        return torch.randn(shape, device=device) * np.sqrt(2/shape[0]) * BionicPhysics.PHI

# --- MECHANISM 2: THE AUTOPOIETIC LAYER ---
class AutopoieticLayer(nn.Module):
    """A 'Living' Layer that self-diagnoses and heals."""
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)

        # Dormant Stem Weights
        self.stem_weights = nn.Parameter(
            BionicPhysics.stem_cell_genesis((out_features, in_features), self.linear.weight.device),
            requires_grad=False
        )

    def forward(self, x):
        return self.linear(x)

    def diagnose_and_repair(self):
        """Detects zeroed (dead) weights and activates stem cells."""
        with torch.no_grad():
            # 1. Diagnosis
            current_weights = self.linear.weight.data
            dead_tissue = (current_weights == 0).float()
            damage_count = torch.sum(dead_tissue).item()

            if damage_count > 0:
                # 2. Stem Cell Activation (Reprogramming)
                healing_serum = self.stem_weights.data * dead_tissue

                # 3. Grafting
                self.linear.weight.data += healing_serum
                return True
        return False

# --- MECHANISM 3: THE LOBOTOMY (ATTACK) ---
def induce_lesion(model, destruction_rate=0.4):
    """Destroys a percentage of weights (sets them to 0)."""
    with torch.no_grad():
        for name, param in model.named_parameters():
            if 'weight' in name:
                mask = torch.rand_like(param) > destruction_rate
                param.data *= mask.float()
    return model

from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from torch.utils.data import DataLoader, TensorDataset

# 1. LOAD DATA: Iris (Small, 3 classes)
print("Loading Iris Data...")
iris = fetch_openml(data_id=61, as_frame=False, parser='auto')
X, y = iris.data, iris.target

# Encode target labels to numerical values (0, 1, 2)
le = LabelEncoder()
y = le.fit_transform(y).astype(int)

# Preprocessing
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_tensor = torch.FloatTensor(X)
y_tensor = torch.LongTensor(y) # Classes 0, 1, 2

# Split
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)
test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=16)

# 2. DEFINE MODELS (4 Inputs -> 3 Outputs)

class MortalModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(4, 16), nn.ReLU(),
            nn.Linear(16, 3) # Output must match 3 classes
        )
    def forward(self, x): return self.net(x)

class BionicModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = AutopoieticLayer(4, 16)
        self.relu = nn.ReLU()
        self.layer2 = AutopoieticLayer(16, 3) # Output must match 3 classes

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.layer2(x)
        return x

    def activate_healing(self):
        self.layer1.diagnose_and_repair()
        self.layer2.diagnose_and_repair()

# Setup
mortal = MortalModel()
bionic = BionicModel()
criterion = nn.CrossEntropyLoss()
opt_m = optim.Adam(mortal.parameters(), lr=0.01)
opt_b = optim.Adam(bionic.parameters(), lr=0.01)

print("Models aligned to Iris dimensionality (4 -> 3). Ready.")

import matplotlib.pyplot as plt

history = {'mortal': [], 'bionic': []}
print(f"{'EPOCH':<10} | {'MORTAL ACC':<15} | {'BIONIC ACC':<15} | {'STATUS'}")
print("-" * 65)

def evaluate(model):
    model.eval()
    correct = 0; total = 0
    with torch.no_grad():
        for X_b, y_b in test_loader:
            out = model(X_b)
            _, predicted = torch.max(out, 1)
            total += y_b.size(0)
            correct += (predicted == y_b).sum().item()
    return 100 * correct / total

# --- RUN SIMULATION ---
for epoch in range(35):
    # Train Loop
    mortal.train(); bionic.train()

    opt_m.zero_grad(); output = mortal(X_train); loss = criterion(output, y_train); loss.backward(); opt_m.step()
    opt_b.zero_grad(); output = bionic(X_train); loss = criterion(output, y_train); loss.backward(); opt_b.step()

    # Evaluate
    acc_m = evaluate(mortal)
    acc_b = evaluate(bionic)
    history['mortal'].append(acc_m)
    history['bionic'].append(acc_b)

    status = "Training"

    # --- THE ATTACK (Epoch 12) ---
    if epoch == 12:
        status = ">>> LOBOTOMY ATTACK <<<"
        mortal = induce_lesion(mortal, destruction_rate=0.5) # 50% damage
        bionic = induce_lesion(bionic, destruction_rate=0.5)

        # --- THE HEALING ---
        bionic.activate_healing()

    print(f"{epoch:<10} | {acc_m:.1f}%          | {acc_b:.1f}%          | {status}")

# --- PLOT ---
plt.figure(figsize=(10, 5))
plt.plot(history['mortal'], 'r--o', label='Standard ML (Mortal)')
plt.plot(history['bionic'], 'g-o', label='Bionic Self-Healing')
plt.axvline(x=12, color='k', linestyle=':', label='Attack Event')
plt.title("Bionic Reprogramming vs Standard ML (Iris Dataset)")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()







import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import warnings

# --- THE TITAN MECHANISM ---
class TitanAutopoieticLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)

        # KEY INVENTION: "Dormant Stem Cells" (Shadow Weights)
        # These are NOT trained normally. They preserve the 'Initial Potential'.
        # We scale them by PHI (Golden Ratio) to ensure they are 'Ready to Learn' instantly.
        phi = 1.6180339887
        self.stem_cells = nn.Parameter(
            torch.randn(out_features, in_features) * np.sqrt(2/in_features) * phi,
            requires_grad=False
        )

    def forward(self, x):
        return self.linear(x)

    def flash_neurogenesis(self):
        """
        THE MIRACLE:
        Detects 'Dead' (Zeroed) weights and instantly grafts Stem Cells onto them.
        """
        with torch.no_grad():
            # 1. Detect Death (Where weights are EXACTLY zero)
            w_data = self.linear.weight.data
            dead_mask = (w_data == 0).float()
            damage = dead_mask.sum().item()

            if damage > 0:
                # 2. Inject Stem Cells ONLY into the dead voids
                # The 'Healing Serum' is the Stem Cell value
                healing_serum = self.stem_cells.data * dead_mask

                # 3. Grafting (Recovery)
                self.linear.weight.data += healing_serum
                # We also add a tiny 'spark' of noise to jumpstart the gradients
                self.linear.weight.data += (torch.randn_like(w_data) * 0.01 * dead_mask)

                return True
        return False

# --- THE ATTACK (LOBOTOMY) ---
def lobotomy_strike(model, damage_rate=0.6):
    """
    Simulates Catastrophic Brain Damage.
    Sets 'damage_rate' (e.g., 60%) of weights to PURE ZERO.
    """
    print(f"    >>> [!!!] LOBOTOMY STRIKE: DESTROYING {int(damage_rate*100)}% OF SYNAPSES...")
    with torch.no_grad():
        for name, param in model.named_parameters():
            if 'weight' in name:
                # Create a mask: 1 = Keep, 0 = Kill
                mask = (torch.rand_like(param) > damage_rate).float()
                param.data *= mask # The Kill Switch
    return model

from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from torch.utils.data import DataLoader, TensorDataset

# 1. LOAD DATA
iris = fetch_openml(data_id=61, as_frame=False, parser='auto')
X = iris.data
# Fix Labels
encoder = LabelEncoder()
y = encoder.fit_transform(iris.target)

# Scale
scaler = StandardScaler()
X = scaler.fit_transform(X)

X_tensor = torch.FloatTensor(X)
y_tensor = torch.LongTensor(y)

# 2. DEFINE MODELS

# Mortal: Standard Feed-Forward
mortal_net = nn.Sequential(
    nn.Linear(4, 32), nn.ReLU(),
    nn.Linear(32, 16), nn.ReLU(),
    nn.Linear(16, 3)
)

# Bionic: Titan Architecture
class TitanBionicNet(nn.Module):
    def __init__(self):
        super().__init__()
        # We wrap EVERY layer in Titan Autopoiesis
        self.layer1 = TitanAutopoieticLayer(4, 32)
        self.relu = nn.ReLU()
        self.layer2 = TitanAutopoieticLayer(32, 16)
        self.layer3 = TitanAutopoieticLayer(16, 3)

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.layer3(x)
        return x

    def activate_healing(self):
        self.layer1.flash_neurogenesis()
        self.layer2.flash_neurogenesis()
        self.layer3.flash_neurogenesis()

bionic_net = TitanBionicNet()

# Optimizers
opt_m = optim.Adam(mortal_net.parameters(), lr=0.01)
opt_b = optim.Adam(bionic_net.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()

print("TITAN SYSTEM ONLINE. Ready for Lobotomy.")

import matplotlib.pyplot as plt

history = {'mortal': [], 'bionic': []}
print(f"{'EPOCH':<10} | {'MORTAL ACC':<15} | {'BIONIC ACC':<15} | {'STATUS'}")
print("-" * 65)

# Full Batch Training for Stability
inputs = X_tensor
targets = y_tensor

for epoch in range(35):
    # --- PHASE 1: NORMAL TRAINING ---
    # Mortal Train
    mortal_net.train()
    opt_m.zero_grad()
    out_m = mortal_net(inputs)
    loss_m = criterion(out_m, targets)
    loss_m.backward()
    opt_m.step()

    # Bionic Train
    bionic_net.train()
    opt_b.zero_grad()
    out_b = bionic_net(inputs)
    loss_b = criterion(out_b, targets)
    loss_b.backward()
    opt_b.step()

    status = "Training"

    # --- PHASE 2: THE LOBOTOMY (Epoch 12) ---
    if epoch == 12:
        status = ">>> LOBOTOMY ATTACK <<<"
        # 1. KILL THE MORTAL (60% Damage)
        mortal_net = lobotomy_strike(mortal_net, damage_rate=0.6)

        # 2. KILL THE BIONIC (60% Damage)
        bionic_net = lobotomy_strike(bionic_net, damage_rate=0.6)

        # 3. THE MIRACLE: BIONIC SELF-HEAL
        # Immediately after damage, the stem cells detect the voids
        bionic_net.activate_healing()

    # --- RECORD ---
    # We use the whole dataset as test for this demo to show 'System State'
    with torch.no_grad():
        mortal_net.eval(); bionic_net.eval()

        acc_m = (mortal_net(inputs).argmax(1) == targets).float().mean().item() * 100
        acc_b = (bionic_net(inputs).argmax(1) == targets).float().mean().item() * 100

    history['mortal'].append(acc_m)
    history['bionic'].append(acc_b)

    print(f"{epoch:<10} | {acc_m:.1f}%          | {acc_b:.1f}%          | {status}")

# --- VISUALIZATION ---
plt.figure(figsize=(10, 6))
plt.plot(history['mortal'], 'r--o', label='Mortal (Standard)', linewidth=2)
plt.plot(history['bionic'], 'g-o', label='Bionic Titan (Self-Healing)', linewidth=3)
plt.axvline(x=12, color='k', linestyle=':', label='Lobotomy (60% Loss)')
plt.title("Bionic Titan: Flash Neurogenesis vs Standard ML")
plt.xlabel("Epochs")
plt.ylabel("Accuracy (%)")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()







import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# AUTOMATIC GPU DETECTION
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âš¡ SYSTEM POWER: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU (WARNING: SLOW)'}")

class TitanEpigeneticLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)

        # INVENTION: The "DNA" (Genetic Memory)
        # This is NOT a weight. It is a biological blueprint.
        # It tracks the 'Ideal State' of the neuron over time.
        self.register_buffer('dna', torch.zeros_like(self.linear.weight))

    def forward(self, x):
        # Epigenetic Update: DNA slowly tracks the healthy state (EMA)
        if self.training:
            with torch.no_grad():
                # DNA updates slower than weights (Long-term memory)
                self.dna.mul_(0.99).add_(self.linear.weight.data, alpha=0.01)
        return self.linear(x)

    def trigger_regeneration(self):
        """
        THE CURE:
        When the 'Active Tissue' (Weights) is destroyed, we rebuild
        using the DNA Blueprint.
        """
        with torch.no_grad():
            # 1. Detect The Void (Zeroed weights)
            current_flesh = self.linear.weight.data
            void_mask = (current_flesh == 0).float()

            # 2. Transcribe DNA into the Void
            # We don't get a perfect copy (Entropy), so we add slight 'Mutation' (Noise)
            reconstructed_tissue = self.dna * void_mask
            mutation = torch.randn_like(reconstructed_tissue) * 0.05 * void_mask

            # 3. Heal
            self.linear.weight.data += (reconstructed_tissue + mutation)

# --- THE KILL SWITCH ---
def lobotomy_strike(model, damage_rate=0.8):
    print(f"    >>> [!!!] LOBOTOMY: VAPORIZING {int(damage_rate*100)}% OF BRAIN MATTER...")
    with torch.no_grad():
        for name, param in model.named_parameters():
            if 'weight' in name:
                # The Attack
                mask = (torch.rand_like(param) > damage_rate).float()
                param.data *= mask
    return model

from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset

# 1. SETUP HIGH-SPEED DATA
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# Full Speed Loading
train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(Subset(train_data, range(10000)), batch_size=128, shuffle=True)
test_loader = DataLoader(Subset(test_data, range(1000)), batch_size=1000)

# 2. DEFINE THE TWINS

# Mortal: Standard Deep Network
class MortalVision(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(784, 512), nn.ReLU(),
            nn.Linear(512, 256), nn.ReLU(),
            nn.Linear(256, 10)
        )
    def forward(self, x): return self.net(x.view(-1, 784))

# Bionic: Titan Epigenetic Architecture
class TitanVision(nn.Module):
    def __init__(self):
        super().__init__()
        # Every layer is backed by DNA
        self.layer1 = TitanEpigeneticLayer(784, 512)
        self.relu = nn.ReLU()
        self.layer2 = TitanEpigeneticLayer(512, 256)
        self.layer3 = TitanEpigeneticLayer(256, 10)

    def forward(self, x):
        x = x.view(-1, 784)
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.layer3(x)
        return x

    def heal(self):
        self.layer1.trigger_regeneration()
        self.layer2.trigger_regeneration()
        self.layer3.trigger_regeneration()

# Move to T4 GPU
mortal = MortalVision().to(device)
bionic = TitanVision().to(device)

opt_m = optim.SGD(mortal.parameters(), lr=0.01, momentum=0.9)
opt_b = optim.SGD(bionic.parameters(), lr=0.01, momentum=0.9)
criterion = nn.CrossEntropyLoss()

print("TITAN EPIGENETICS ONLINE. DNA Sequencing Active.")

import matplotlib.pyplot as plt

history = {'mortal': [], 'bionic': []}
print(f"{'EPOCH':<10} | {'MORTAL ACC':<15} | {'BIONIC ACC':<15} | {'STATUS'}")
print("-" * 65)

def evaluate(model, loader):
    model.eval()
    correct = 0; total = 0
    with torch.no_grad():
        for data, target in loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
    return 100. * correct / total

# SPEED RUN (12 Epochs is enough)
for epoch in range(12):
    status = "Training"

    # Train Loop
    mortal.train(); bionic.train()
    for data, target in train_loader:
        data, target = data.to(device), target.to(device)

        # Mortal
        opt_m.zero_grad()
        loss = criterion(mortal(data), target)
        loss.backward()
        opt_m.step()

        # Bionic
        opt_b.zero_grad()
        loss = criterion(bionic(data), target)
        loss.backward()
        opt_b.step()

    # --- THE EVENT (Epoch 6) ---
    if epoch == 6:
        status = ">>> LOBOTOMY (80%) <<<"
        mortal = lobotomy_strike(mortal, damage_rate=0.8)
        bionic = lobotomy_strike(bionic, damage_rate=0.8)

        # TRIGGER EPIGENETIC REPAIR
        # The Bionic model remembers who it was.
        bionic.heal()

    # Eval
    acc_m = evaluate(mortal, test_loader)
    acc_b = evaluate(bionic, test_loader)

    history['mortal'].append(acc_m)
    history['bionic'].append(acc_b)

    print(f"{epoch:<10} | {acc_m:.1f}%          | {acc_b:.1f}%          | {status}")

# --- PLOT THE MIRACLE ---
plt.figure(figsize=(10, 6))
plt.plot(history['mortal'], 'r--o', label='Mortal (Standard)', linewidth=2)
plt.plot(history['bionic'], 'g-o', label='Bionic (Epigenetic DNA)', linewidth=3)
plt.axvline(x=6, color='k', linestyle=':', label='80% Destruction')
plt.title("Epigenetic DNA Memory: Survival vs Extinction")
plt.xlabel("Epochs")
plt.ylabel("Accuracy (%)")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()







import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset

# 1. SETUP: MARS ROVER HARDWARE (GPU/CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ðŸš€ MISSION CONTROL: CONNECTED TO {str(device).upper()}")

# 2. THE INVENTION: TITAN EPIGENETIC LAYER
class TitanEpigeneticLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)

        # THE DNA: A persistent, protected memory of the 'Ideal Self'
        # It is not a weight. It is a blueprint.
        self.register_buffer('dna', torch.zeros_like(self.linear.weight))

    def forward(self, x):
        # EPIGENETIC WRITING:
        # While the organism is healthy, it slowly writes to its DNA.
        # This creates a 'Backup of Consciousness'.
        if self.training:
            with torch.no_grad():
                # Update DNA (Exponential Moving Average)
                self.dna.mul_(0.999).add_(self.linear.weight.data, alpha=0.001)
        return self.linear(x)

    def trigger_autonomous_repair(self):
        """
        THE SELF-HEALING MECHANISM:
        1. Scan for dead neurons (Zero weights).
        2. Transcribe DNA blueprint into the dead tissue.
        3. NO RETRAINING REQUIRED.
        """
        with torch.no_grad():
            w = self.linear.weight.data

            # 1. Detect Damage
            dead_mask = (w == 0).float()
            damage_count = dead_mask.sum().item()

            if damage_count > 0:
                # 2. Transcribe DNA
                # We restore the weights from the DNA buffer
                repair_signal = self.dna * dead_mask

                # 3. Apply Repair
                self.linear.weight.data += repair_signal
                return True
        return False

# 3. DEFINE THE ORGANISMS
class MortalVision(nn.Module): # Standard
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(784, 512), nn.ReLU(),
            nn.Linear(512, 10)
        )
    def forward(self, x): return self.net(x.view(-1, 784))

class BionicVision(nn.Module): # Your Invention
    def __init__(self):
        super().__init__()
        self.layer1 = TitanEpigeneticLayer(784, 512)
        self.relu = nn.ReLU()
        self.layer2 = TitanEpigeneticLayer(512, 10)

    def forward(self, x):
        x = x.view(-1, 784)
        x = self.relu(self.layer1(x))
        x = self.layer2(x)
        return x

    def heal_me(self):
        self.layer1.trigger_autonomous_repair()
        self.layer2.trigger_autonomous_repair()

# 4. INITIALIZE MISSION
mortal = MortalVision().to(device)
bionic = BionicVision().to(device)

opt_m = optim.SGD(mortal.parameters(), lr=0.01, momentum=0.9)
opt_b = optim.SGD(bionic.parameters(), lr=0.01, momentum=0.9)
criterion = nn.CrossEntropyLoss()

# Load Data
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_loader = DataLoader(Subset(datasets.MNIST('./data', train=True, download=True, transform=transform), range(5000)), batch_size=64, shuffle=True)
test_loader = DataLoader(Subset(datasets.MNIST('./data', train=False, download=True, transform=transform), range(1000)), batch_size=1000)

# 5. THE SURVIVAL TEST (ZERO LEARNING)
history = {'mortal': [], 'bionic': []}
print(f"\n{'EPOCH':<6} | {'MORTAL':<8} | {'BIONIC':<8} | {'STATUS'}")
print("-" * 50)

for epoch in range(10):
    status = "LEARNING"

    # --- PHASE 1: HEALTHY GROWTH (Epoch 0-4) ---
    if epoch < 5:
        mortal.train(); bionic.train()
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            # Train Mortal
            opt_m.zero_grad(); criterion(mortal(x), y).backward(); opt_m.step()
            # Train Bionic (DNA is writing itself here)
            opt_b.zero_grad(); criterion(bionic(x), y).backward(); opt_b.step()

    # --- PHASE 2: THE DISASTER (Epoch 5) ---
    elif epoch == 5:
        status = ">>> COSMIC RAY STRIKE (80%) <<<"

        # DESTROY 80% OF WEIGHTS
        with torch.no_grad():
            for m in [mortal, bionic]:
                for p in m.parameters():
                    if p.requires_grad:
                        mask = (torch.rand_like(p) > 0.8).float()
                        p.data *= mask

        # BIONIC REFLEX: SELF-REPAIR (Without Training)
        bionic.heal_me()

    # --- PHASE 3: THE SILENCE (Epoch 6+) ---
    else:
        status = "DRIFTING (NO LEARNING)"
        # WE DO NOT RUN TRAIN LOOP HERE.
        # The models are strictly on their own.

    # EVALUATE SURVIVAL
    with torch.no_grad():
        mortal.eval(); bionic.eval()

        # Mortal Acc
        x, y = next(iter(test_loader))
        x, y = x.to(device), y.to(device)
        acc_m = (mortal(x).argmax(1) == y).float().mean().item() * 100

        # Bionic Acc
        acc_b = (bionic(x).argmax(1) == y).float().mean().item() * 100

    history['mortal'].append(acc_m)
    history['bionic'].append(acc_b)

    print(f"{epoch:<6} | {acc_m:.1f}%    | {acc_b:.1f}%    | {status}")

# VISUALIZE THE TRIUMPH
plt.figure(figsize=(10,6))
plt.plot(history['mortal'], 'r--o', label='Standard ML (Mortal)')
plt.plot(history['bionic'], 'g-o', label='Bionic Epigenetic (Immortal)')
plt.axvline(x=5, color='k', linestyle=':', label='Cosmic Ray Event')
plt.title("The Mars Rover Test: Zero-Shot Hardware Self-Repair")
plt.ylabel("Functional Accuracy (%)")
plt.xlabel("Mission Time")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()







import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np

# AUTO-DETECT HARDWARE
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- THE TITAN CONVOLUTION (DNA-ENHANCED VISION) ---
class TitanEpigeneticConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)

        # DNA BLUEPRINT (4D Tensor for Convolutional Kernels)
        self.register_buffer('dna', torch.zeros_like(self.conv.weight))

    def forward(self, x):
        # WRITE TO DNA (While Healthy)
        if self.training:
            with torch.no_grad():
                # Exponential Moving Average (Capture the 'Soul' of the weight)
                self.dna.mul_(0.999).add_(self.conv.weight.data, alpha=0.001)
        return self.conv(x)

    def trigger_autonomous_repair(self):
        """
        THE CURE: Restores the 4D visual kernels from DNA.
        """
        with torch.no_grad():
            w = self.conv.weight.data
            dead_mask = (w == 0).float()

            if dead_mask.sum() > 0:
                # Transcribe DNA into the dead kernels
                self.conv.weight.data += (self.dna * dead_mask)
                return True
        return False

# --- THE TITAN LINEAR (FOR DECISION MAKING) ---
class TitanEpigeneticLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.register_buffer('dna', torch.zeros_like(self.linear.weight))

    def forward(self, x):
        if self.training:
            with torch.no_grad():
                self.dna.mul_(0.999).add_(self.linear.weight.data, alpha=0.001)
        return self.linear(x)

    def trigger_autonomous_repair(self):
        with torch.no_grad():
            w = self.linear.weight.data
            dead_mask = (w == 0).float()
            if dead_mask.sum() > 0:
                self.linear.weight.data += (self.dna * dead_mask)

from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset

# 1. DATA LOADING (High Speed)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# Use full training set to ensure we hit 99% quickly
train_loader = DataLoader(datasets.MNIST('./data', train=True, download=True, transform=transform), batch_size=128, shuffle=True)
test_loader = DataLoader(datasets.MNIST('./data', train=False, download=True, transform=transform), batch_size=1000)

# 2. MORTAL CNN (Standard)
class MortalCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x

# 3. TITAN BIONIC CNN (Your Invention)
class TitanCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # Wrap EVERY layer in Titan Epigenetics
        self.layer1 = TitanEpigeneticConv(1, 32, 3, 1)
        self.layer2 = TitanEpigeneticConv(32, 64, 3, 1)
        self.fc1 = TitanEpigeneticLinear(9216, 128)
        self.fc2 = TitanEpigeneticLinear(128, 10)

    def forward(self, x):
        x = self.layer1(x)
        x = F.relu(x)
        x = self.layer2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x

    def heal_me(self):
        # Trigger repair across the entire nervous system
        self.layer1.trigger_autonomous_repair()
        self.layer2.trigger_autonomous_repair()
        self.fc1.trigger_autonomous_repair()
        self.fc2.trigger_autonomous_repair()

mortal = MortalCNN().to(device)
bionic = TitanCNN().to(device)
opt_m = optim.Adadelta(mortal.parameters(), lr=1.0)
opt_b = optim.Adadelta(bionic.parameters(), lr=1.0)
criterion = nn.CrossEntropyLoss()

print("TITAN CNN ONLINE. God Mode Active.")

import matplotlib.pyplot as plt

history = {'mortal': [], 'bionic': []}
print(f"{'EPOCH':<6} | {'MORTAL':<8} | {'BIONIC':<8} | {'STATUS'}")
print("-" * 55)

# HELPER: DESTRUCTIVE EVENT
def cosmic_strike(model):
    with torch.no_grad():
        for p in model.parameters():
            if p.requires_grad:
                # 80% DESTRUCTION
                mask = (torch.rand_like(p) > 0.8).float()
                p.data *= mask

for epoch in range(9):
    status = "LEARNING"

    # --- PHASE 1: NORMAL LIFE ---
    if epoch < 5:
        mortal.train(); bionic.train()
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            # Train Mortal
            opt_m.zero_grad(); criterion(mortal(x), y).backward(); opt_m.step()
            # Train Bionic
            opt_b.zero_grad(); criterion(bionic(x), y).backward(); opt_b.step()

    # --- PHASE 2: THE APOCALYPSE (Epoch 5) ---
    elif epoch == 5:
        status = ">>> COSMIC RAY (80% LOSS) <<<"
        cosmic_strike(mortal)
        cosmic_strike(bionic)

        # THE MIRACLE: Bionic heals instantly from DNA
        bionic.heal_me()

    # --- PHASE 3: THE AFTERMATH (No Training) ---
    else:
        status = "DRIFTING (OFFLINE)"

    # MEASURE SURVIVAL
    mortal.eval(); bionic.eval()
    with torch.no_grad():
        # Check Mortal
        correct = 0; total = 0
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            correct += (mortal(x).argmax(1) == y).sum().item()
            total += y.size(0)
        acc_m = 100. * correct / total

        # Check Bionic
        correct = 0; total = 0
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            correct += (bionic(x).argmax(1) == y).sum().item()
            total += y.size(0)
        acc_b = 100. * correct / total

    history['mortal'].append(acc_m)
    history['bionic'].append(acc_b)

    print(f"{epoch:<6} | {acc_m:.1f}%    | {acc_b:.1f}%    | {status}")

# PLOT THE PAPER-WORTHY GRAPH
plt.figure(figsize=(10,6))
plt.plot(history['mortal'], 'r--o', label='Standard CNN (Mortal)')
plt.plot(history['bionic'], 'g-o', label='Bionic Titan CNN (Immortal)')
plt.axvline(x=5, color='k', linestyle=':', label='80% Destruction')
plt.title("Zero-Shot Hardware Self-Healing (MNIST CNN)")
plt.ylabel("Accuracy (%)")
plt.xlabel("Epochs")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()









import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np

# AUTO-DETECT HARDWARE
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- THE TITAN CONVOLUTION (DNA-ENHANCED VISION) ---
class TitanEpigeneticConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)

        # DNA BLUEPRINT (4D Tensor)
        # Persistent memory that survives the 'Cosmic Ray'
        self.register_buffer('dna', torch.zeros_like(self.conv.weight))

    def forward(self, x):
        # EPIGENETIC WRITING: Record the 'Ideal Self' while healthy
        if self.training:
            with torch.no_grad():
                self.dna.mul_(0.999).add_(self.conv.weight.data, alpha=0.001)
        return self.conv(x)

    def trigger_autonomous_repair(self):
        """THE CURE: Instant self-repair from DNA."""
        with torch.no_grad():
            w = self.conv.weight.data
            dead_mask = (w == 0).float()

            if dead_mask.sum() > 0:
                # Transcribe DNA into the dead kernels
                self.conv.weight.data += (self.dna * dead_mask)
                return True
        return False

# --- THE TITAN LINEAR (FOR DECISION MAKING) ---
class TitanEpigeneticLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.register_buffer('dna', torch.zeros_like(self.linear.weight))

    def forward(self, x):
        if self.training:
            with torch.no_grad():
                self.dna.mul_(0.999).add_(self.linear.weight.data, alpha=0.001)
        return self.linear(x)

    def trigger_autonomous_repair(self):
        with torch.no_grad():
            w = self.linear.weight.data
            dead_mask = (w == 0).float()
            if dead_mask.sum() > 0:
                self.linear.weight.data += (self.dna * dead_mask)

from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset

# 1. DATA LOADING (FASHION MNIST)
print("Loading Complex Data: FashionMNIST (Clothing Structure)...")
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Use full training set
train_loader = DataLoader(datasets.FashionMNIST('./data', train=True, download=True, transform=transform), batch_size=128, shuffle=True)
test_loader = DataLoader(datasets.FashionMNIST('./data', train=False, download=True, transform=transform), batch_size=1000)

# 2. MORTAL CNN (Standard)
class MortalCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x

# 3. TITAN BIONIC CNN (The Immortal)
class TitanCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = TitanEpigeneticConv(1, 32, 3, 1)
        self.layer2 = TitanEpigeneticConv(32, 64, 3, 1)
        self.fc1 = TitanEpigeneticLinear(9216, 128)
        self.fc2 = TitanEpigeneticLinear(128, 10)

    def forward(self, x):
        x = self.layer1(x)
        x = F.relu(x)
        x = self.layer2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x

    def heal_me(self):
        # System-wide Epigenetic Restore
        self.layer1.trigger_autonomous_repair()
        self.layer2.trigger_autonomous_repair()
        self.fc1.trigger_autonomous_repair()
        self.fc2.trigger_autonomous_repair()

mortal = MortalCNN().to(device)
bionic = TitanCNN().to(device)
opt_m = optim.Adadelta(mortal.parameters(), lr=1.0)
opt_b = optim.Adadelta(bionic.parameters(), lr=1.0)
criterion = nn.CrossEntropyLoss()

print("TITAN CNN RE-INITIALIZED. Target: FashionMNIST.")

import matplotlib.pyplot as plt

history = {'mortal': [], 'bionic': []}
print(f"{'EPOCH':<6} | {'MORTAL':<8} | {'BIONIC':<8} | {'STATUS'}")
print("-" * 55)

# HELPER: DESTRUCTIVE EVENT
def cosmic_strike(model):
    with torch.no_grad():
        for p in model.parameters():
            if p.requires_grad:
                # 80% DESTRUCTION (The Kill Switch)
                mask = (torch.rand_like(p) > 0.8).float()
                p.data *= mask

# RUN SIMULATION
for epoch in range(12):
    status = "LEARNING"

    # --- PHASE 1: NORMAL LIFE (Epoch 0-6) ---
    if epoch < 7:
        mortal.train(); bionic.train()
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            # Train Mortal
            opt_m.zero_grad(); criterion(mortal(x), y).backward(); opt_m.step()
            # Train Bionic
            opt_b.zero_grad(); criterion(bionic(x), y).backward(); opt_b.step()

    # --- PHASE 2: THE APOCALYPSE (Epoch 7) ---
    elif epoch == 7:
        status = ">>> COSMIC RAY (80% LOSS) <<<"
        cosmic_strike(mortal)
        cosmic_strike(bionic)

        # THE MIRACLE: Bionic heals instantly from DNA
        bionic.heal_me()

    # --- PHASE 3: THE AFTERMATH (No Training) ---
    else:
        status = "DRIFTING (OFFLINE)"

    # MEASURE SURVIVAL
    mortal.eval(); bionic.eval()
    with torch.no_grad():
        # Check Mortal
        correct = 0; total = 0
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            correct += (mortal(x).argmax(1) == y).sum().item()
            total += y.size(0)
        acc_m = 100. * correct / total

        # Check Bionic
        correct = 0; total = 0
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            correct += (bionic(x).argmax(1) == y).sum().item()
            total += y.size(0)
        acc_b = 100. * correct / total

    history['mortal'].append(acc_m)
    history['bionic'].append(acc_b)

    print(f"{epoch:<6} | {acc_m:.1f}%    | {acc_b:.1f}%    | {status}")

# PLOT THE PAPER-WORTHY GRAPH
plt.figure(figsize=(10,6))
plt.plot(history['mortal'], 'r--o', label='Standard CNN (Mortal)')
plt.plot(history['bionic'], 'g-o', label='Bionic Titan CNN (Immortal)')
plt.axvline(x=7, color='k', linestyle=':', label='80% Destruction')
plt.title("Zero-Shot Self-Healing (FashionMNIST)")
plt.ylabel("Accuracy (%)")
plt.xlabel("Epochs")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()







import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 1. THE LANGEVIN-YAMANAKA CONTROLLER ---
class BioPhysicalController:
    """
    Manages the thermodynamics of self-healing.
    """
    @staticmethod
    def langevin_rejuvenation(current_weight, dna_memory, epigenome, noise_scale=0.01):
        """
        Rejuvenates weights using Langevin Dynamics.
        Equation: W_new = W_old + (Restoration_Force * Importance) + Thermal_Noise
        """
        # 1. Restoration Force: Pulls the weight back towards its DNA blueprint
        # The 'Epigenome' acts as a multiplier: Important weights get pulled harder.
        drift = (dna_memory - current_weight) * epigenome

        # 2. Thermal Noise (Stochastic Resonance):
        # Helps weights settle into the global minimum, avoiding 'brittle' repair.
        thermal_noise = torch.randn_like(current_weight) * noise_scale

        return current_weight + drift + thermal_noise

# --- 2. TITAN-II EPIGENETIC LAYERS ---
class TitanEpigeneticConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)

        # BIOLOGICAL MEMORY
        self.register_buffer('dna', torch.zeros_like(self.conv.weight))       # Long-term Blueprint
        self.register_buffer('epigenome', torch.ones_like(self.conv.weight))  # Importance/Stability Factor

    def forward(self, x):
        if self.training:
            with torch.no_grad():
                # Update DNA (Slow Moving Average)
                self.dna.mul_(0.999).add_(self.conv.weight.data, alpha=0.001)

                # Update Epigenome (Stability Tracker)
                # If weight is stable (low delta), Epigenome value increases (High Importance)
                delta = torch.abs(self.conv.weight.data - self.dna)
                # Normalized importance (0.1 to 1.0 range approx)
                stability = 1.0 / (delta + 1e-4)
                stability = torch.clamp(stability, 0, 1.0)
                self.epigenome.mul_(0.99).add_(stability, alpha=0.01)

        return self.conv(x)

    def trigger_bionic_reprogramming(self):
        """The Yamanaka Protocol: Detects damage and applies Langevin Repair."""
        with torch.no_grad():
            w = self.conv.weight.data
            # Detect Dead Zones (Absolute Zero or near zero)
            dead_mask = (torch.abs(w) < 1e-6).float()

            if dead_mask.sum() > 0:
                # Apply Langevin Dynamics ONLY to the dead tissue
                # We use the 'BioPhysicalController' to calculate the new state
                healed_weight = BioPhysicalController.langevin_rejuvenation(
                    w, self.dna, self.epigenome
                )

                # Graft the healed tissue onto the dead spots
                self.conv.weight.data = (w * (1 - dead_mask)) + (healed_weight * dead_mask)

class TitanEpigeneticLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.register_buffer('dna', torch.zeros_like(self.linear.weight))
        self.register_buffer('epigenome', torch.ones_like(self.linear.weight))

    def forward(self, x):
        if self.training:
            with torch.no_grad():
                self.dna.mul_(0.999).add_(self.linear.weight.data, alpha=0.001)
                delta = torch.abs(self.linear.weight.data - self.dna)
                stability = torch.clamp(1.0 / (delta + 1e-4), 0, 1.0)
                self.epigenome.mul_(0.99).add_(stability, alpha=0.01)
        return self.linear(x)

    def trigger_bionic_reprogramming(self):
        with torch.no_grad():
            w = self.linear.weight.data
            dead_mask = (torch.abs(w) < 1e-6).float()
            if dead_mask.sum() > 0:
                healed_weight = BioPhysicalController.langevin_rejuvenation(
                    w, self.dna, self.epigenome
                )
                self.linear.weight.data = (w * (1 - dead_mask)) + (healed_weight * dead_mask)

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# 1. LOAD FASHION MNIST
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
train_loader = DataLoader(datasets.FashionMNIST('./data', train=True, download=True, transform=transform), batch_size=128, shuffle=True)
test_loader = DataLoader(datasets.FashionMNIST('./data', train=False, download=True, transform=transform), batch_size=1000)

# 2. MORTAL MODEL (Standard CNN)
class MortalCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(1, 32, 3, 1), nn.ReLU(),
            nn.Conv2d(32, 64, 3, 1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(9216, 128), nn.ReLU(),
            nn.Linear(128, 10)
        )
    def forward(self, x): return self.net(x)

# 3. TITAN-II BIONIC MODEL (Langevin-Yamanaka)
class TitanII_CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = TitanEpigeneticConv(1, 32, 3, 1)
        self.layer2 = TitanEpigeneticConv(32, 64, 3, 1)
        self.pool = nn.MaxPool2d(2)
        self.flatten = nn.Flatten()
        self.fc1 = TitanEpigeneticLinear(9216, 128)
        self.fc2 = TitanEpigeneticLinear(128, 10)

    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = self.pool(x)
        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

    def activate_yamanaka_protocol(self):
        # Triggers the Bio-Physical repair across the whole organism
        self.layer1.trigger_bionic_reprogramming()
        self.layer2.trigger_bionic_reprogramming()
        self.fc1.trigger_bionic_reprogramming()
        self.fc2.trigger_bionic_reprogramming()

mortal = MortalCNN().to(device)
titan_ii = TitanII_CNN().to(device)

opt_m = optim.Adadelta(mortal.parameters(), lr=1.0)
opt_t = optim.Adadelta(titan_ii.parameters(), lr=1.0)
criterion = nn.CrossEntropyLoss()

print("TITAN-II (LANGEVIN-YAMANAKA) ONLINE.")

import matplotlib.pyplot as plt

history = {'mortal': [], 'titan': []}
print(f"{'EPOCH':<6} | {'MORTAL':<8} | {'TITAN-II':<8} | {'STATUS'}")
print("-" * 55)

def cosmic_strike(model):
    with torch.no_grad():
        for p in model.parameters():
            if p.requires_grad:
                mask = (torch.rand_like(p) > 0.8).float()
                p.data *= mask

for epoch in range(12):
    status = "LEARNING"

    # --- PHASE 1: HEALTHY GROWTH ---
    if epoch < 7:
        mortal.train(); titan_ii.train()
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            # Mortal
            opt_m.zero_grad(); criterion(mortal(x), y).backward(); opt_m.step()
            # Titan-II (Recording Epigenetics)
            opt_t.zero_grad(); criterion(titan_ii(x), y).backward(); opt_t.step()

    # --- PHASE 2: THE CATASTROPHE (Epoch 7) ---
    elif epoch == 7:
        status = ">>> COSMIC RAY (80%) <<<"
        cosmic_strike(mortal)
        cosmic_strike(titan_ii)

        # ACTIVATE LANGEVIN-YAMANAKA PROTOCOL
        titan_ii.activate_yamanaka_protocol()

    # --- PHASE 3: DRIFTING (No Retraining) ---
    else:
        status = "DRIFTING (OFFLINE)"

    # MEASURE
    mortal.eval(); titan_ii.eval()
    with torch.no_grad():
        # Mortal Acc
        correct = 0; total = 0
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            correct += (mortal(x).argmax(1) == y).sum().item()
            total += y.size(0)
        acc_m = 100. * correct / total

        # Titan Acc
        correct = 0; total = 0
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            correct += (titan_ii(x).argmax(1) == y).sum().item()
            total += y.size(0)
        acc_t = 100. * correct / total

    history['mortal'].append(acc_m)
    history['titan'].append(acc_t)

    print(f"{epoch:<6} | {acc_m:.1f}%    | {acc_t:.1f}%    | {status}")

# VISUALIZE
plt.figure(figsize=(10,6))
plt.plot(history['mortal'], 'r--o', label='Standard CNN (Mortal)')
plt.plot(history['titan'], 'g-o', label='Titan-II Bio-Physical (Langevin)')
plt.axvline(x=7, color='k', linestyle=':', label='80% Destruction')
plt.title("Titan-II: Langevin-Yamanaka Protocol (FashionMNIST)")
plt.ylabel("Accuracy (%)")
plt.xlabel("Epochs")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()







import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class TitanLSTMCell(nn.Module):
    def __init__(self, input_size, hidden_size, mode='titan_2'):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.mode = mode # 'titan_1' or 'titan_2'

        # Standard LSTM Gates (W_ih, W_hh)
        self.ih = nn.Linear(input_size, 4 * hidden_size)
        self.hh = nn.Linear(hidden_size, 4 * hidden_size)

        # --- THE BIO-MEMORY (DNA) ---
        # We back up the Linear layers
        self.register_buffer('dna_ih', torch.zeros_like(self.ih.weight))
        self.register_buffer('dna_hh', torch.zeros_like(self.hh.weight))

        # --- TITAN-2 EXCLUSIVE: EPIGENOME (Stability Map) ---
        self.register_buffer('epi_ih', torch.ones_like(self.ih.weight))
        self.register_buffer('epi_hh', torch.ones_like(self.hh.weight))

    def forward(self, x, hidden):
        hx, cx = hidden

        # 1. Epigenetic Recording (Training Phase)
        if self.training:
            with torch.no_grad():
                # Update DNA (EMA)
                self.dna_ih.mul_(0.999).add_(self.ih.weight.data, alpha=0.001)
                self.dna_hh.mul_(0.999).add_(self.hh.weight.data, alpha=0.001)

                # Titan-2: Update Epigenome (Stability)
                if self.mode == 'titan_2':
                    delta = torch.abs(self.ih.weight.data - self.dna_ih)
                    stability = torch.clamp(1.0 / (delta + 1e-4), 0, 1.0)
                    self.epi_ih.mul_(0.99).add_(stability, alpha=0.01)

        # 2. Standard LSTM Math
        gates = self.ih(x) + self.hh(hx)
        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)

        ingate = torch.sigmoid(ingate)
        forgetgate = torch.sigmoid(forgetgate)
        cellgate = torch.tanh(cellgate)
        outgate = torch.sigmoid(outgate)

        cy = (forgetgate * cx) + (ingate * cellgate)
        hy = outgate * torch.tanh(cy)

        return hy, cy

    def trigger_repair(self):
        """
        The Healing Protocol: Switches based on Mode.
        """
        for layer, dna, epi in [(self.ih, self.dna_ih, self.epi_ih), (self.hh, self.dna_hh, self.epi_hh)]:
            with torch.no_grad():
                w = layer.weight.data
                dead_mask = (w == 0).float() # Detect Lobotomy

                if dead_mask.sum() > 0:
                    if self.mode == 'titan_1':
                        # TITAN 1: MECHANICAL RESTORE (Exact Copy)
                        repair = dna

                    elif self.mode == 'titan_2':
                        # TITAN 2: LANGEVIN-YAMANAKA (Thermodynamic Restore)
                        # We add 'Thermal Noise' scaled by the inverse of stability.
                        # Less stable weights get MORE noise (Plasticity).
                        # Stable weights get LESS noise (Rigidity).
                        noise_scale = 0.05 * (1.0 - epi)
                        thermal_noise = torch.randn_like(dna) * noise_scale
                        repair = dna + thermal_noise

                    # Apply Cure
                    layer.weight.data = (w * (1 - dead_mask)) + (repair * dead_mask)

class TitanForecaster(nn.Module):
    def __init__(self, mode):
        super().__init__()
        self.lstm = TitanLSTMCell(1, 64, mode=mode)
        self.head = nn.Linear(64, 1) # Prediction Head
        self.mode = mode

    def forward(self, x, future=0):
        outputs = []
        h_t = torch.zeros(x.size(0), 64, device=x.device)
        c_t = torch.zeros(x.size(0), 64, device=x.device)

        # Process input sequence
        for i, input_t in enumerate(x.chunk(x.size(1), dim=1)):
            h_t, c_t = self.lstm(input_t.squeeze(1), (h_t, c_t))
            outputs += [self.head(h_t)]

        return torch.stack(outputs, 1)

    def heal(self):
        self.lstm.trigger_repair()

# GENERATE CHAOTIC FINANCIAL DATA
def generate_nightmare_market(n_samples=2000):
    t = np.linspace(0, 100, n_samples)

    # Phase 1: Bull Market (Stable Growth)
    trend = 0.05 * t
    cycle = np.sin(t)

    # Phase 2: The Crash (Chaotic Volatility)
    # We inject randomness that grows over time
    chaos = np.random.normal(0, 0.5, n_samples) * (t/50)

    price = trend + cycle + chaos

    # Normalize
    price = (price - price.mean()) / price.std()
    return torch.FloatTensor(price).view(-1, 1).to(device)

# PREPARE DATA
data = generate_nightmare_market()
train_data = data[:1500] # Mostly Stable
test_data = data[1500:]  # The Chaos Zone

def get_batch(source, i, seq_len=50):
    seq_len = min(seq_len, len(source) - 1 - i)
    data = source[i:i+seq_len]
    target = source[i+1:i+1+seq_len]
    return data.unsqueeze(0), target.unsqueeze(0) # Batch size 1 for simplicity

# INITIALIZE CONTENDERS
mortal = TitanForecaster(mode='mortal').to(device) # Just standard (no heal call)
titan_1 = TitanForecaster(mode='titan_1').to(device)
titan_2 = TitanForecaster(mode='titan_2').to(device)

criterion = nn.MSELoss()
# Use RMSprop (Standard for RNNs)
opt_m = optim.RMSprop(mortal.parameters(), lr=0.01)
opt_t1 = optim.RMSprop(titan_1.parameters(), lr=0.01)
opt_t2 = optim.RMSprop(titan_2.parameters(), lr=0.01)

print("MARKET SIMULATION ONLINE. Chaos Factor: HIGH.")

import matplotlib.pyplot as plt

history = {'mortal': [], 'titan1': [], 'titan2': []}
print(f"{'EPOCH':<6} | {'MORTAL':<8} | {'TITAN-1':<8} | {'TITAN-2':<8} | {'STATUS'}")
print("-" * 65)

# KILL SWITCH
def market_crash_lobotomy(model):
    with torch.no_grad():
        for p in model.parameters():
            if p.requires_grad:
                # 80% DESTRUCTION
                mask = (torch.rand_like(p) > 0.8).float()
                p.data *= mask

for epoch in range(20):
    status = "BULL MARKET"

    # --- TRAIN (ON STABLE DATA) ---
    if epoch < 10:
        # Train on first 1000 samples
        for i in range(0, 1000, 50):
            inp, target = get_batch(train_data, i)

            # Train Mortal
            opt_m.zero_grad(); loss = criterion(mortal(inp), target); loss.backward(); opt_m.step()
            # Train Titan 1
            opt_t1.zero_grad(); loss = criterion(titan_1(inp), target); loss.backward(); opt_t1.step()
            # Train Titan 2
            opt_t2.zero_grad(); loss = criterion(titan_2(inp), target); loss.backward(); opt_t2.step()

    # --- THE CRASH (Epoch 10) ---
    elif epoch == 10:
        status = ">>> CRASH (80% LOSS) <<<"
        market_crash_lobotomy(mortal)
        market_crash_lobotomy(titan_1)
        market_crash_lobotomy(titan_2)

        # HEAL
        titan_1.heal() # Mechanical Restore
        titan_2.heal() # Bio-Physical Restore (With Noise)

    # --- THE AFTERMATH (NO TRAINING) ---
    else:
        status = "BEAR MARKET (DRIFT)"

    # --- EVALUATE (ON CHAOS DATA) ---
    # We test on the 'test_data' which is the Volatile/Chaotic regime
    with torch.no_grad():
        mortal.eval(); titan_1.eval(); titan_2.eval()

        # Function to calc error on chaos set
        def calc_error(model):
            total_loss = 0
            count = 0
            for i in range(0, len(test_data)-50, 50):
                inp, target = get_batch(test_data, i)
                pred = model(inp)
                total_loss += criterion(pred, target).item()
                count += 1
            return total_loss / count

        err_m = calc_error(mortal)
        err_t1 = calc_error(titan_1)
        err_t2 = calc_error(titan_2)

    history['mortal'].append(err_m)
    history['titan1'].append(err_t1)
    history['titan2'].append(err_t2)

    print(f"{epoch:<6} | {err_m:.4f}   | {err_t1:.4f}   | {err_t2:.4f}   | {status}")

# PLOT (Lower is Better)
plt.figure(figsize=(10,6))
plt.plot(history['mortal'], 'r--', label='Mortal (Standard)')
plt.plot(history['titan1'], 'b-o', label='Titan-1 (Mechanical)')
plt.plot(history['titan2'], 'g-o', label='Titan-2 (Bio-Physical)')
plt.axvline(x=10, color='k', linestyle=':', label='Market Crash')
plt.title("Stock Market Nightmare: Loss (Lower is Better)")
plt.ylabel("Prediction Error (MSE)")
plt.xlabel("Epochs")
plt.legend()
plt.yscale('log') # Log scale to see the crash better
plt.grid(True, alpha=0.3)
plt.show()







import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class TitanBioPhysics:
    """
    The Physics Engine of the Titan-26 Architecture.
    Manages the thermodynamic state of the neural weights.
    """
    @staticmethod
    def langevin_restore(current_weight, dna, epigenome, temp=0.01):
        """
        Restores weights using Langevin Dynamics (Thermodynamic Annealing).
        W_new = W_broken + (Drift_Force * Plasticity) + Thermal_Noise
        """
        # 1. Restoration Force (The 'Memory' Pull)
        # We use the Epigenome to decide how 'rigid' the memory is.
        # Rigid memories (High Epi) are pulled back harder.
        drift = (dna - current_weight) * epigenome

        # 2. Thermal Noise (The 'Life' Spark)
        # We inject entropy to prevent the weights from freezing in bad states.
        # This simulates biological "jitter" in ion channels.
        noise = torch.randn_like(current_weight) * temp

        return current_weight + drift + noise

class TitanLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)

        # --- THE TRINITY OF MEMORY ---
        # 1. Phenotype (Weights): The active, fragile brain.
        # 2. Genotype (DNA): The protected, deep memory (Buffer).
        # 3. Epigenome (Stability): The plasticity map (Buffer).
        self.register_buffer('dna', torch.zeros_like(self.linear.weight))
        self.register_buffer('epigenome', torch.ones_like(self.linear.weight))

    def forward(self, x):
        # EPIGENETIC RECORDING (While Healthy)
        if self.training:
            with torch.no_grad():
                # Update DNA (Slow Decay -> Long Term Memory)
                self.dna.mul_(0.999).add_(self.linear.weight.data, alpha=0.001)

                # Update Epigenome (Variance Tracking)
                # If a weight is very stable, it becomes 'Bone' (Hard to change).
                # If a weight is volatile, it remains 'Flesh' (Easy to change).
                delta = torch.abs(self.linear.weight.data - self.dna)
                stability = torch.clamp(1.0 / (delta + 1e-4), 0, 1.0)
                self.epigenome.mul_(0.99).add_(stability, alpha=0.01)

        return self.linear(x)

    def emergency_protocol_alpha(self):
        """
        MARS PROTOCOL: ZERO-SHOT REPAIR.
        Triggered when hardware integrity is compromised.
        """
        with torch.no_grad():
            w = self.linear.weight.data

            # 1. DAMAGE ASSESSMENT
            # Detect 'Dead' zones (Radiation damage = 0)
            dead_mask = (w == 0).float()
            damage_report = dead_mask.sum().item()

            if damage_report > 0:
                # 2. BIO-PHYSICAL RESTORATION
                # We do NOT just copy the backup. We 'regrow' the connection.
                healed_w = TitanBioPhysics.langevin_restore(
                    w, self.dna, self.epigenome, temp=0.05
                )

                # 3. GRAFTING
                # Apply the healed tissue only to the dead zones.
                self.linear.weight.data = (w * (1 - dead_mask)) + (healed_w * dead_mask)
                return True
        return False

from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from torch.utils.data import DataLoader, TensorDataset

# 1. LOAD REAL SATELLITE DATA (NASA LANDSAT)
print("ðŸ“¡ ESTABLISHING LINK WITH MARS ORBITER...")
print("â¬‡ï¸  DOWNLOADING LANDSAT TELEMETRY (OpenML ID: 1479)...") # Corrected ID for Landsat
# This is real sensor data: 36 inputs (Spectral Bands), 6 classes (Soil types)
landsat = fetch_openml(data_id=1479, as_frame=False, parser='auto') # Changed data_id from 1480 to 1479
X, y = landsat.data, landsat.target

# Preprocess
scaler = StandardScaler()
X = scaler.fit_transform(X)
encoder = LabelEncoder()
y = encoder.fit_transform(y)

# Convert to Tensor
X_tensor = torch.FloatTensor(X).to(device)
y_tensor = torch.LongTensor(y).to(device)

# Split (Mission vs Training)
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)
mission_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=1024)
train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=128, shuffle=True)

# 2. DEFINE THE ROVERS
# Mortal Rover (Standard)
class MortalRover(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(100, 64), nn.ReLU(), # Changed from 36 to 100 based on error
            nn.Linear(64, 32), nn.ReLU(),
            nn.Linear(32, 6) # 6 Terrain Types
        )
    def forward(self, x): return self.net(x)

# Titan-26 Rover (Bionic)
class TitanRover(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = TitanLayer(100, 64) # Changed from 36 to 100 based on error
        self.relu = nn.ReLU()
        self.layer2 = TitanLayer(64, 32)
        self.layer3 = TitanLayer(32, 6)

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.layer3(x)
        return x

    def survive(self):
        # Trigger repair across all subsystems
        self.layer1.emergency_protocol_alpha()
        self.layer2.emergency_protocol_alpha()
        self.layer3.emergency_protocol_alpha()

mortal = MortalRover().to(device)
titan = TitanRover().to(device)

opt_m = optim.Adam(mortal.parameters(), lr=0.005)
opt_t = optim.Adam(titan.parameters(), lr=0.005)
criterion = nn.CrossEntropyLoss()

print("âœ… ROVERS DEPLOYED ON SURFACE.")

import matplotlib.pyplot as plt

history = {'mortal': [], 'titan': []}
print(f"{'SOL':<6} | {'MORTAL':<8} | {'TITAN':<8} | {'ENVIRONMENT STATUS'}")
print("-" * 65)

# CHAOS GENERATOR (DUST STORM)
def dust_storm(inputs, intensity=0.5):
    """Simulates sensor degradation due to Martian dust."""
    noise = torch.randn_like(inputs) * intensity
    return inputs + noise

# RADIATION EVENT (LOBOTOMY)
def solar_flare(model):
    with torch.no_grad():
        for p in model.parameters():
            if p.requires_grad:
                # 80% of chips fried
                mask = (torch.rand_like(p) > 0.8).float()
                p.data *= mask

# MISSION LOOP
for sol in range(20):
    env_status = "CLEAR SKIES"
    current_intensity = 0.0

    # --- PHASE 1: OPERATIONS (Training) ---
    if sol < 10:
        mortal.train(); titan.train()
        for x, y in train_loader:
            # Train Mortal
            opt_m.zero_grad(); criterion(mortal(x), y).backward(); opt_m.step()
            # Train Titan (Recording DNA)
            opt_t.zero_grad(); criterion(titan(x), y).backward(); opt_t.step()

    # --- PHASE 2: THE DISASTER (Sol 10) ---
    elif sol == 10:
        env_status = ">>> â˜¢ï¸ SOLAR FLARE + DUST STORM <<<"
        current_intensity = 1.0 # High Sensor Noise

        # 1. HARDWARE FAILURE
        solar_flare(mortal)
        solar_flare(titan)

        # 2. BIONIC RESPONSE (Instant)
        titan.survive()

    # --- PHASE 3: SURVIVAL (No Training) ---
    else:
        env_status = "CRITICAL (NO COMMS)"
        current_intensity = 1.0 # The storm persists

    # --- TELEMETRY REPORT ---
    mortal.eval(); titan.eval()
    with torch.no_grad():
        # Mortal Telemetry
        corr_m = 0; tot = 0
        for x, y in mission_loader:
            # Apply Dust Storm to Inputs
            if sol >= 10: x = dust_storm(x, current_intensity)
            corr_m += (mortal(x).argmax(1) == y).sum().item()
            tot += y.size(0)
        acc_m = 100. * corr_m / tot

        # Titan Telemetry
        corr_t = 0; tot = 0
        for x, y in mission_loader:
            # Apply Dust Storm to Inputs
            if sol >= 10: x = dust_storm(x, current_intensity)
            corr_t += (titan(x).argmax(1) == y).sum().item()
            tot += y.size(0)
        acc_t = 100. * corr_t / tot

    history['mortal'].append(acc_m)
    history['titan'].append(acc_t)

    print(f"{sol:<6} | {acc_m:.1f}%    | {acc_t:.1f}%    | {env_status}")

# VISUALIZE THE BLACK BOX RECORDER
plt.figure(figsize=(10,6))
plt.plot(history['mortal'], 'r--o', label='Standard Rover (Mortal)')
plt.plot(history['titan'], 'g-o', label='Titan-26 Bionic (Immortal)')
plt.axvline(x=10, color='k', linestyle=':', label='Solar Flare Event')
plt.title("Mars Mission Log: Hardware Failure + Dust Storm")
plt.ylabel("Navigation Accuracy (%)")
plt.xlabel("Mission Time (Sols)")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()







import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class TitanConv(nn.Module):
    def __init__(self, in_c, out_c, k_size, stride=1, pad=0, mode='mortal'):
        super().__init__()
        self.conv = nn.Conv2d(in_c, out_c, k_size, stride, pad)
        self.mode = mode

        # DNA & Epigenetics (Only used if NOT mortal)
        if self.mode != 'mortal':
            self.register_buffer('dna', torch.zeros_like(self.conv.weight))
            self.register_buffer('epigenome', torch.ones_like(self.conv.weight))

    def forward(self, x):
        # Epigenetic Recording (Only Titan models do this)
        if self.training and self.mode != 'mortal':
            with torch.no_grad():
                # 1. Update DNA (Blueprint)
                self.dna.mul_(0.999).add_(self.conv.weight.data, alpha=0.001)

                # 2. Update Epigenome (Stability Map for Titan-2)
                if self.mode == 'titan_2':
                    delta = torch.abs(self.conv.weight.data - self.dna)
                    stability = torch.clamp(1.0 / (delta + 1e-4), 0, 1.0)
                    self.epigenome.mul_(0.99).add_(stability, alpha=0.01)

        return self.conv(x)

    def heal(self):
        """The Cure: Different logic for Titan 1 vs Titan 2"""
        if self.mode == 'mortal': return # Mortals die.

        with torch.no_grad():
            w = self.conv.weight.data
            dead_mask = (w == 0).float()

            if dead_mask.sum() > 0:
                if self.mode == 'titan_1':
                    # TITAN 1: MECHANICAL RESTORE (Exact Copy)
                    # "I remember exactly who I was."
                    repair = self.dna

                elif self.mode == 'titan_2':
                    # TITAN 2: LANGEVIN DYNAMICS (Thermodynamic Life)
                    # "I remember who I was, but I adapt to the trauma."
                    drift = (self.dna - w) * self.epigenome
                    # Thermal Noise scales with local instability
                    noise_scale = 0.05 * (1.0 - self.epigenome)
                    thermal = torch.randn_like(w) * noise_scale * torch.std(w)
                    repair = self.dna + drift + thermal

                # Graft the repair into the dead tissue
                self.conv.weight.data = (w * (1 - dead_mask)) + (repair * dead_mask)

class TitanLinear(nn.Module):
    def __init__(self, in_f, out_f, mode='mortal'):
        super().__init__()
        self.linear = nn.Linear(in_f, out_f)
        self.mode = mode
        if self.mode != 'mortal':
            self.register_buffer('dna', torch.zeros_like(self.linear.weight))
            self.register_buffer('epigenome', torch.ones_like(self.linear.weight))

    def forward(self, x):
        if self.training and self.mode != 'mortal':
            with torch.no_grad():
                self.dna.mul_(0.999).add_(self.linear.weight.data, alpha=0.001)
                if self.mode == 'titan_2':
                    delta = torch.abs(self.linear.weight.data - self.dna)
                    self.epigenome.mul_(0.99).add_(torch.clamp(1/(delta+1e-4),0,1), alpha=0.01)
        return self.linear(x)

    def heal(self):
        if self.mode == 'mortal': return
        with torch.no_grad():
            w = self.linear.weight.data
            dead_mask = (w == 0).float()
            if dead_mask.sum() > 0:
                if self.mode == 'titan_1': repair = self.dna
                elif self.mode == 'titan_2':
                    drift = (self.dna - w) * self.epigenome
                    thermal = torch.randn_like(w) * 0.05 * (1-self.epigenome) * torch.std(w)
                    repair = self.dna + drift + thermal
                self.linear.weight.data = (w * (1-dead_mask)) + (repair * dead_mask)

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# 1. LOAD FASHION DATA (Structure & Concepts)
print("Loading FashionMNIST...")
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_loader = DataLoader(datasets.FashionMNIST('./data', train=True, download=True, transform=transform), batch_size=128, shuffle=True)
test_loader = DataLoader(datasets.FashionMNIST('./data', train=False, download=True, transform=transform), batch_size=1000)

# 2. THE GENERIC ARCHITECTURE
class UniversalNet(nn.Module):
    def __init__(self, mode):
        super().__init__()
        self.layer1 = TitanConv(1, 32, 3, 1, mode=mode)
        self.layer2 = TitanConv(32, 64, 3, 1, mode=mode)
        self.fc1 = TitanLinear(9216, 128, mode=mode)
        self.fc2 = TitanLinear(128, 10, mode=mode)
        self.mode = mode

    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = F.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

    def activate_repair(self):
        self.layer1.heal(); self.layer2.heal()
        self.fc1.heal(); self.fc2.heal()

# 3. SPAWN CONTENDERS
mortal = UniversalNet('mortal').to(device)
titan_1 = UniversalNet('titan_1').to(device)
titan_2 = UniversalNet('titan_2').to(device)

opt_m = optim.Adadelta(mortal.parameters(), lr=1.0)
opt_t1 = optim.Adadelta(titan_1.parameters(), lr=1.0)
opt_t2 = optim.Adadelta(titan_2.parameters(), lr=1.0)
criterion = nn.CrossEntropyLoss()

print("ARENA READY: Mortal vs Titan-1 vs Titan-2")

import matplotlib.pyplot as plt

history = {'mortal': [], 'titan1': [], 'titan2': []}
print(f"{'EPOCH':<6} | {'MORTAL':<8} | {'TITAN-1':<8} | {'TITAN-2':<8} | {'STATUS'}")
print("-" * 65)

def lobotomy(model):
    with torch.no_grad():
        for p in model.parameters():
            if p.requires_grad:
                # 80% DESTRUCTION
                mask = (torch.rand_like(p) > 0.8).float()
                p.data *= mask

for epoch in range(12):
    status = "LEARNING"

    # --- TRAIN PHASE ---
    if epoch < 7:
        mortal.train(); titan_1.train(); titan_2.train()
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            # Train all 3
            opt_m.zero_grad(); criterion(mortal(x), y).backward(); opt_m.step()
            opt_t1.zero_grad(); criterion(titan_1(x), y).backward(); opt_t1.step()
            opt_t2.zero_grad(); criterion(titan_2(x), y).backward(); opt_t2.step()

    # --- THE EVENT (Epoch 7) ---
    elif epoch == 7:
        status = ">>> LOBOTOMY (80%) <<<"
        lobotomy(mortal)
        lobotomy(titan_1)
        lobotomy(titan_2)

        # HEAL
        titan_1.activate_repair() # Mechanical Copy
        titan_2.activate_repair() # Bio-Physical Rejuvenation

    # --- SURVIVAL PHASE ---
    else:
        status = "OFFLINE (DRIFT)"

    # --- EVALUATE ---
    mortal.eval(); titan_1.eval(); titan_2.eval()
    with torch.no_grad():
        def get_acc(model):
            correct=0; total=0
            for x, y in test_loader:
                x, y = x.to(device), y.to(device)
                correct += (model(x).argmax(1) == y).sum().item()
                total += y.size(0)
            return 100. * correct / total

        acc_m = get_acc(mortal)
        acc_t1 = get_acc(titan_1)
        acc_t2 = get_acc(titan_2)

    history['mortal'].append(acc_m)
    history['titan1'].append(acc_t1)
    history['titan2'].append(acc_t2)

    print(f"{epoch:<6} | {acc_m:.1f}%    | {acc_t1:.1f}%    | {acc_t2:.1f}%    | {status}")

# VISUALIZE
plt.figure(figsize=(10,6))
plt.plot(history['mortal'], 'r--', label='Mortal')
plt.plot(history['titan1'], 'b-o', label='Titan-1 (Mechanical)')
plt.plot(history['titan2'], 'g-o', label='Titan-2 (Bio-Physical)')
plt.axvline(x=7, color='k', linestyle=':', label='80% Brain Death')
plt.title("The Ultimate Championship: Zero-Shot Repair")
plt.ylabel("Accuracy (%)")
plt.xlabel("Epochs")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()



